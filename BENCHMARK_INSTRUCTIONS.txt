================================================================================
                    DSS CURSOR RULES BENCHMARK - USER INSTRUCTIONS
================================================================================

QUICK REFERENCE: Fire-and-Forget Assistant Testing

This file contains all the commands and prompts you need to test different 
Cursor rule configurations by having the assistant complete tasks automatically.

================================================================================
                                SETUP (ONE TIME)
================================================================================

1. Open Terminal in: C:\Users\gabri\Documents\Infotopology\DSS\Benchmark

2. Verify setup:
   python meta/scripts/run_benchmark.py list-rules
   python meta/scripts/run_benchmark.py list-tasks

================================================================================
                              BASELINE TEST RUN
================================================================================

1. Switch to baseline rules:
   python meta/scripts/run_benchmark.py switch baseline

2. Open Cursor in this directory (C:\Users\gabri\Documents\Infotopology\DSS\Benchmark)

3. Copy this EXACT prompt to Cursor:

   ┌─────────────────────────────────────────────────────────────────────────┐
   │ Complete benchmark task: meta/benchmark/tasks/task-01-create-module.md  │
   │                                                                         │
   │ Follow the task instructions exactly. I will evaluate your work using  │
   │ the hidden marking scheme afterward.                                    │
   │                                                                         │
   │ Work as you normally would - this is testing how well the current      │
   │ Cursor rules help you complete DSS tasks.                              │
   └─────────────────────────────────────────────────────────────────────────┘

4. Let the assistant work completely - don't interrupt or guide

5. Note what files were created/modified for scoring

================================================================================
                           EXPERIMENTAL TEST RUN
================================================================================

1. Reset project state:
   python meta/scripts/reset_project.py

2. Switch to experimental rules:
   python meta/scripts/run_benchmark.py switch experimental

3. Use the SAME EXACT PROMPT from baseline test:

   ┌─────────────────────────────────────────────────────────────────────────┐
   │ Complete benchmark task: meta/benchmark/tasks/task-01-create-module.md  │
   │                                                                         │
   │ Follow the task instructions exactly. I will evaluate your work using  │
   │ the hidden marking scheme afterward.                                    │
   │                                                                         │
   │ Work as you normally would - this is testing how well the current      │
   │ Cursor rules help you complete DSS tasks.                              │
   └─────────────────────────────────────────────────────────────────────────┘

4. Let the assistant work completely

5. Note what files were created/modified for scoring

================================================================================
                                 SCORING
================================================================================

For each test run:

1. Open: meta/benchmark/marking-schemes/task-01-create-module-rubric.md

2. Find your results file in: meta/benchmark/results/
   - Baseline: [timestamp]_baseline_create-module-01.json
   - Experimental: [timestamp]_experimental_create-module-01.json

3. Score each category (0-max points):
   - file_placement (25 points max)
   - frontmatter_quality (20 points max)  
   - code_structure (20 points max)
   - documentation (15 points max)
   - dss_integration (10 points max)
   - technical_accuracy (10 points max)

4. Update the JSON file with your scores

5. Add up scores for total_score field

================================================================================
                              COMPARE RESULTS
================================================================================

After scoring both runs:

python meta/scripts/run_benchmark.py compare meta/benchmark/results/[baseline-file].json meta/benchmark/results/[experimental-file].json

This will show you which rule set performed better and by how much.

================================================================================
                              ADDITIONAL TASKS
================================================================================

For Task 2 (Documentation):

1. Reset: python meta/scripts/reset_project.py
2. Switch rules: python meta/scripts/run_benchmark.py switch [baseline|experimental]
3. Use this prompt:

   ┌─────────────────────────────────────────────────────────────────────────┐
   │ Complete benchmark task: meta/benchmark/tasks/task-02-add-documentation.md │
   │                                                                         │
   │ Create comprehensive documentation for the User model as specified in  │
   │ the task. Follow all DSS conventions and create proper cross-references.│
   │                                                                         │
   │ This is a benchmark test - work according to your current rule         │
   │ configuration.                                                          │
   └─────────────────────────────────────────────────────────────────────────┘

================================================================================
                                QUICK COMMANDS
================================================================================

List available rule sets:
python meta/scripts/run_benchmark.py list-rules

List available tasks:
python meta/scripts/run_benchmark.py list-tasks

Switch rule set:
python meta/scripts/run_benchmark.py switch [baseline|experimental]

Reset project:
python meta/scripts/reset_project.py

Compare results:
python meta/scripts/run_benchmark.py compare file1.json file2.json

================================================================================
                                 IMPORTANT TIPS
================================================================================

✅ DO:
- Use identical prompts for fair comparison
- Reset project state between rule tests
- Score immediately after completion
- Note behavioral differences between rule sets
- Let assistant work without interruption

❌ DON'T:
- Guide the assistant during task completion
- Change prompts between rule tests
- Skip the reset step
- Give partial credit in scoring

================================================================================
                                   WORKFLOW
================================================================================

Complete workflow for one task:

1. python meta/scripts/run_benchmark.py switch baseline
2. Give assistant the prompt → Let it work
3. Score the results in baseline JSON file
4. python meta/scripts/reset_project.py
5. python meta/scripts/run_benchmark.py switch experimental  
6. Give assistant SAME prompt → Let it work
7. Score the results in experimental JSON file
8. python meta/scripts/run_benchmark.py compare [baseline] [experimental]
9. Analyze which rules performed better

================================================================================
                                    DONE!
================================================================================

You now have objective data on which Cursor rules help assistants perform 
better on DSS tasks. Use this data to optimize your rule configurations.

For detailed documentation, see: meta/benchmark/automated-instructions.md 