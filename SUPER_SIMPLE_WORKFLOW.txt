================================================================================
                         ULTRA-SIMPLE BENCHMARK WORKFLOW
================================================================================

You are NOT the LLM anymore! The assistant does EVERYTHING.

================================================================================
                                   WORKFLOW
================================================================================

BASELINE TEST:
1. python meta\scripts\run_benchmark.py start baseline create-module-01
2. Copy the prompt from: meta/benchmark/FULLY_AUTOMATED_PROMPT.txt
3. Paste into Cursor â†’ Assistant does everything (task + scoring)

EXPERIMENTAL TEST:  
1. python meta\scripts\reset_project.py
2. python meta\scripts\run_benchmark.py start experimental create-module-01
3. Copy the SAME prompt from: meta/benchmark/FULLY_AUTOMATED_PROMPT.txt  
4. Paste into Cursor â†’ Assistant does everything (task + scoring)

COMPARE:
python meta\scripts\run_benchmark.py compare meta\benchmark\results\*baseline*.json meta\benchmark\results\*experimental*.json

================================================================================
                                 THAT'S IT!
================================================================================

The assistant will:
âœ… Read the task requirements
âœ… Complete the implementation  
âœ… Read the marking scheme
âœ… Score its own work objectively
âœ… Fill out the results JSON
âœ… Give you a summary

You just switch rules and paste prompts. ðŸŽ¯

================================================================================
                              EVEN SIMPLER
================================================================================

Want to test both rule sets quickly?

1. python meta\scripts\run_benchmark.py start baseline create-module-01
2. Give assistant the automated prompt â†’ Wait for completion
3. python meta\scripts\reset_project.py
4. python meta\scripts\run_benchmark.py start experimental create-module-01  
5. Give assistant the same prompt â†’ Wait for completion
6. python meta\scripts\run_benchmark.py compare meta\benchmark\results\*baseline*.json meta\benchmark\results\*experimental*.json

Done! You have objective data on which rules perform better.

================================================================================ 